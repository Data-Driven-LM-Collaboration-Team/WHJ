### A-MEM背景

当前LLM Agent通常使用简单的记忆机制，例如，对话缓冲区（Conversation Buffer）: 存储近期对话，但很快会因上下文窗口限制而丢失早期信息；向量存储（Vector Stores）：将文本块（chunks）嵌入为向量并进行相似性检索。这虽然解决了长期存储问题，但其记忆组织是零散和静态的，难以捕捉记忆之间的复杂、高阶关系。 图数据库（Graph Databases）: 如Mem0等工作尝试使用图结构，但其模式和关系类型通常需要预定义，限制了系统在面对全新、未知任务时的适应性。

MEM核心目标：设计一个灵活、通用的记忆系统，使其能够像人类一样，随着经验的积累，不断地自我组织、形成新的连接，并深化对已有知识的理解。

### 核心思想

A-Mem的核心思想借鉴了一种非常高效的个人知识管理方法——（Zettelkasten）卡片盒笔记法。

原子化：每个知识点记在一张独立的卡片上。

建立链接：不断思考新卡片和旧卡片之间的联系，并用线把它们连起来。

形成网络：最终，所有知识不再是一堆孤立的文件，而是一个相互连接的、有机生长的网络。

Mem就是把AI的每一次交互和学到的东西，都变成这样一张张“智能卡片”，并让AI自己来为这些卡片建立链接。

<img width="1894" height="250" alt="image" src="https://github.com/user-attachments/assets/4badc332-ea84-48cc-982f-7c8aeac5f97b" />

1.  笔记构建

当agent与环境发生一次新的交互时，A-Mem不会只存储原始对话，它会调用一个大语言模型来为这次经历构建一个结构化的记忆笔记（智能卡片）。随后，系统将所有文本信息（原始内容和LLM生成的元数据）拼接起来，通过文本编码器生成一个浓缩的嵌入向量。

2.  链接生成

向量检索找出相似度最高的top-k个记忆构成“最近邻候选集”，系统将新笔记和候选集一同提供给LLM分析深层次的联系，判断是否应该建立链接。

3.  记忆演化

当为新笔记创建链接后，系统会进一步判断新笔记的加入是否应该触发其邻近记忆（来自于最近邻候选集）的更新。LLM会分析新笔记、候选集以及临近记忆，然后决定是否更新以及如何更新临近记忆，在生成一个进化版的临近记忆后将会替换掉原来的临近记忆。

<img width="3164" height="1504" alt="image" src="https://github.com/user-attachments/assets/8b0585d9-e59e-4d66-98b8-034a2f79b99c" />

### 对比基线

MemGPT：复刻操作系统的内存管理逻辑，构建核心记忆（即时上下文）、对话记忆、归档记忆的三层结构。通过类似虚拟内存的"页面置换"机制，模型能在固定上下文窗口内管理无限信息。构建客服机器人、个人助理的热门框架。

多跳推理任务（GPT-4o-mini）中，A-MEM与MemGPT分数对比：45.85：25.52，每次交互的Token消耗对比：17000：1200

MemoryBank：基于人类遗忘曲线构建被动衰减的存储模型。遵循艾宾浩斯遗忘曲线，根据记忆强度动态淘汰信息。
