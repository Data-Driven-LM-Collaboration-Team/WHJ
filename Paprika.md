## LLM现有问题

现有的语言模型虽然在语言理解和生成方面表现出色，但生活里很多事需要多轮探索，比如丢失了快递，得先问快递员有没有派件记录，再根据记录查是不是送错小区，再针对性找物业——这是一个“行动→看反馈→调整行动”的循环。LLM在这种需要多轮交互、主动探索环境的任务中表现笨拙。

#### 真实交互数据太少且危险

要教AI探索，得让它在真实环境里试错，而在真实环境中试错成本较高，且真实场景的交互数据特别乱，用户描述并不一定得当准确，AI很难从中提取关键信息。

#### 传统训练是“死记硬背”，不会迁移

之前有研究给AI练“多臂老虎机”（类似选哪个按钮中奖率高），练得再好，换个“猜单词”任务，AI又变回原样。就像学生只会背数学题，换个物理题就懵了——它没学会“通用的探索方法”。

<img width="1106" height="452" alt="image" src="https://github.com/user-attachments/assets/9cbf03e9-8a88-4f20-821b-db673b118832" />

## Paprika核心思路

#### 任务设计：多样化的交互环境

Paprika 设计了 10 个不同类型的文本交互任务，每个任务都需要模型通过多轮交互来获取信息并做出决策。

20问/猜城市：通过连续提问并观察回答，尽可能快速地猜出答案。

Wordle/Mastermind：猜一个5字单词或4位密码，AI要根据反馈调整。

扫雷：在网格中推理地雷位置

任务共同点：文本交互、部分可观测（模型无法直接看到答案）、需要多轮决策、策略多样（不同任务需要不同策略）

#### 数据生成：自我对弈 + 轨迹采样

有了游戏，下一步就是让AI自己玩，并录下其“游戏过程”（交互轨迹），再从中挑“好的”（快速完成任务）和“差的”（失败或步数过多）当教材。为了鼓励多样性，使用Min-p采样，不允许AI总选最熟悉的答案，得偶尔尝试新选项。

#### 优化方法：RPO（强化偏好优化）

结合监督微调（SFT）和直接偏好优化（DPO）的方法，避免AI走极端。

SFT：让模型模仿成功轨迹

DPO：让模型区分好坏轨迹，偏好更高效的策略（理解为什么好）

只使用SFT会导致AI只会“依葫芦画瓢”，换个任务就不会了，而PAPRIKA结合了DPO，相当于让AI悟出其中的门道。

#### 课程学习：选择“学习潜力大”的任务

与预训练中模型通常能在任意样本上取得进展不同，RL agent若未收集优质元数据或先验知识经验，便无法实现实质性进步。Paprika 提出一种课程学习策略，动态地选择哪些任务用于训练。其核心指标是变异系数：

这个值越高，说明任务中模型的表现差异大，则学习潜力也越大。

如在20问游戏中，有的能8步猜中，有的20步没猜中，说明该任务有学习空间，优先训练；在玩扫雷时全部踩雷（坏轨迹），说明该任务太难了，先放一放。

等AI练会了简单的，再逐步挑战难的——这样既不浪费数据，也不让AI因为总失败而“放弃”。
