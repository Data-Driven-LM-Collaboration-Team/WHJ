{
  "model": "qwen2.5-3b-instruct",
  "dataset": "/home/seki/AgenticMemory/data/locomo10.json",
  "total_questions": 199,
  "category_distribution": {
    "2": 37,
    "3": 13,
    "1": 32,
    "4": 70,
    "5": 47
  },
  "aggregate_metrics": {
    "overall": {
      "exact_match": {
        "mean": 0.07035175879396985,
        "std": 0.25638389950162377,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 199
      },
      "f1": {
        "mean": 0.19938942822126673,
        "std": 0.30629662587931744,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge1_f": {
        "mean": 0.2345436804150627,
        "std": 0.31017274535877193,
        "median": 0.09523809523809525,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge2_f": {
        "mean": 0.1347275573846169,
        "std": 0.27796847412576564,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rougeL_f": {
        "mean": 0.23101239128465084,
        "std": 0.3099004797886784,
        "median": 0.0851063829787234,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "bleu1": {
        "mean": 0.16761086698051217,
        "std": 0.28258245716573777,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu2": {
        "mean": 0.11878818548510545,
        "std": 0.24996121931576326,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu3": {
        "mean": 0.10463523516745549,
        "std": 0.24404847352225625,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu4": {
        "mean": 0.08676327779660627,
        "std": 0.21713844365128526,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bert_precision": {
        "mean": 0.8645659131021356,
        "std": 0.055777506893093236,
        "median": 0.8608695268630981,
        "min": 0.7303819060325623,
        "max": 1.0000001192092896,
        "count": 199
      },
      "bert_recall": {
        "mean": 0.8673104302367972,
        "std": 0.057674530231694505,
        "median": 0.8549909591674805,
        "min": 0.7209846377372742,
        "max": 1.0000001192092896,
        "count": 199
      },
      "bert_f1": {
        "mean": 0.8654140057276242,
        "std": 0.052897634424969545,
        "median": 0.8590729832649231,
        "min": 0.748896062374115,
        "max": 1.0000001192092896,
        "count": 199
      },
      "meteor": {
        "mean": 0.17013025708481452,
        "std": 0.2943175841595541,
        "median": 0.0,
        "min": 0.0,
        "max": 0.9993141289437586,
        "count": 199
      },
      "sbert_similarity": {
        "mean": 0.41482022672016455,
        "std": 0.3052945732295921,
        "median": 0.35358157753944397,
        "min": -0.060942042618989944,
        "max": 1.0000001192092896,
        "count": 199
      }
    },
    "category_1": {
      "exact_match": {
        "mean": 0.0625,
        "std": 0.24593468841898236,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 32
      },
      "f1": {
        "mean": 0.18214685739887354,
        "std": 0.28671557607099907,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 32
      },
      "rouge1_f": {
        "mean": 0.1860388423710792,
        "std": 0.2871258921028243,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 32
      },
      "rouge2_f": {
        "mean": 0.03742516444129348,
        "std": 0.11546550494107545,
        "median": 0.0,
        "min": 0.0,
        "max": 0.5714285714285715,
        "count": 32
      },
      "rougeL_f": {
        "mean": 0.1860388423710792,
        "std": 0.2871258921028243,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 32
      },
      "bleu1": {
        "mean": 0.1541114983184581,
        "std": 0.2630704892146274,
        "median": 0.012867283065077366,
        "min": 0,
        "max": 1.0,
        "count": 32
      },
      "bleu2": {
        "mean": 0.06395812559123878,
        "std": 0.112821279599749,
        "median": 0.004112509279471686,
        "min": 0,
        "max": 0.40451991747794525,
        "count": 32
      },
      "bleu3": {
        "mean": 0.04543781465511983,
        "std": 0.08397994018322501,
        "median": 0.002975318964373025,
        "min": 0,
        "max": 0.301194211912202,
        "count": 32
      },
      "bleu4": {
        "mean": 0.03434448844763388,
        "std": 0.06794008852776649,
        "median": 0.002363679125165153,
        "min": 0,
        "max": 0.301194211912202,
        "count": 32
      },
      "bert_precision": {
        "mean": 0.8696273099631071,
        "std": 0.05435182701556231,
        "median": 0.8717365562915802,
        "min": 0.7812505960464478,
        "max": 1.0,
        "count": 32
      },
      "bert_recall": {
        "mean": 0.8554087560623884,
        "std": 0.05240366629170304,
        "median": 0.8486607074737549,
        "min": 0.7687082290649414,
        "max": 1.0,
        "count": 32
      },
      "bert_f1": {
        "mean": 0.8620444927364588,
        "std": 0.04985959137035186,
        "median": 0.8611381351947784,
        "min": 0.7825846672058105,
        "max": 1.0,
        "count": 32
      },
      "meteor": {
        "mean": 0.08580830631758798,
        "std": 0.15375824121898973,
        "median": 0.0,
        "min": 0.0,
        "max": 0.5,
        "count": 32
      },
      "sbert_similarity": {
        "mean": 0.4039437788305804,
        "std": 0.2626057849119115,
        "median": 0.391860231757164,
        "min": -0.05635596439242363,
        "max": 1.0000001192092896,
        "count": 32
      }
    },
    "category_2": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0,
        "min": 0,
        "max": 0,
        "count": 37
      },
      "f1": {
        "mean": 0.06156156156156156,
        "std": 0.146860015526544,
        "median": 0.0,
        "min": 0.0,
        "max": 0.5,
        "count": 37
      },
      "rouge1_f": {
        "mean": 0.19817509817509815,
        "std": 0.21306714639613575,
        "median": 0.2222222222222222,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 37
      },
      "rouge2_f": {
        "mean": 0.09044109044109044,
        "std": 0.1655298893954236,
        "median": 0.0,
        "min": 0.0,
        "max": 0.5714285714285715,
        "count": 37
      },
      "rougeL_f": {
        "mean": 0.19817509817509815,
        "std": 0.21306714639613575,
        "median": 0.2222222222222222,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 37
      },
      "bleu1": {
        "mean": 0.04856775270780605,
        "std": 0.11708123166523147,
        "median": 0,
        "min": 0,
        "max": 0.42857142857142855,
        "count": 37
      },
      "bleu2": {
        "mean": 0.02161803335612602,
        "std": 0.06240752864310522,
        "median": 0,
        "min": 0,
        "max": 0.26726124191242434,
        "count": 37
      },
      "bleu3": {
        "mean": 0.011829194580474885,
        "std": 0.030366232515627044,
        "median": 0,
        "min": 0,
        "max": 0.12111838948674812,
        "count": 37
      },
      "bleu4": {
        "mean": 0.00950955938895427,
        "std": 0.02323454159737148,
        "median": 0,
        "min": 0,
        "max": 0.08633400213704504,
        "count": 37
      },
      "bert_precision": {
        "mean": 0.8240901231765747,
        "std": 0.04938172328140505,
        "median": 0.8217763900756836,
        "min": 0.7303819060325623,
        "max": 0.9035173654556274,
        "count": 37
      },
      "bert_recall": {
        "mean": 0.8440723757486086,
        "std": 0.04289575107747468,
        "median": 0.8453108072280884,
        "min": 0.7522878646850586,
        "max": 0.939624547958374,
        "count": 37
      },
      "bert_f1": {
        "mean": 0.8334704573090012,
        "std": 0.04167169889733154,
        "median": 0.8321543335914612,
        "min": 0.748896062374115,
        "max": 0.9212172627449036,
        "count": 37
      },
      "meteor": {
        "mean": 0.024039290486658905,
        "std": 0.06440816608584717,
        "median": 0.0,
        "min": 0.0,
        "max": 0.3125,
        "count": 37
      },
      "sbert_similarity": {
        "mean": 0.5400395365180196,
        "std": 0.2031919311071268,
        "median": 0.5809721350669861,
        "min": 0.11275410652160645,
        "max": 0.8403059840202332,
        "count": 37
      }
    },
    "category_3": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0,
        "min": 0,
        "max": 0,
        "count": 13
      },
      "f1": {
        "mean": 0.030722743766222027,
        "std": 0.0647333065706047,
        "median": 0.0,
        "min": 0.0,
        "max": 0.1739130434782609,
        "count": 13
      },
      "rouge1_f": {
        "mean": 0.058216203356180954,
        "std": 0.07837023795162039,
        "median": 0.0,
        "min": 0.0,
        "max": 0.23999999999999996,
        "count": 13
      },
      "rouge2_f": {
        "mean": 0.006688963210702342,
        "std": 0.02411739983587953,
        "median": 0.0,
        "min": 0.0,
        "max": 0.08695652173913045,
        "count": 13
      },
      "rougeL_f": {
        "mean": 0.04396519121043197,
        "std": 0.05662192844846526,
        "median": 0.0,
        "min": 0.0,
        "max": 0.16,
        "count": 13
      },
      "bleu1": {
        "mean": 0.04278549905305249,
        "std": 0.049878654883258725,
        "median": 0.03125,
        "min": 0,
        "max": 0.13793103448275862,
        "count": 13
      },
      "bleu2": {
        "mean": 0.011872597306196405,
        "std": 0.0123382479979065,
        "median": 0.010040241611281232,
        "min": 0,
        "max": 0.029774566708770676,
        "count": 13
      },
      "bleu3": {
        "mean": 0.007456895184325143,
        "std": 0.007656098157404615,
        "median": 0.007306361129019465,
        "min": 0,
        "max": 0.018733084686676876,
        "count": 13
      },
      "bleu4": {
        "mean": 0.005689069284303904,
        "std": 0.005956165317229961,
        "median": 0.005834347180338519,
        "min": 0,
        "max": 0.01553712569276035,
        "count": 13
      },
      "bert_precision": {
        "mean": 0.834829316689418,
        "std": 0.023355681222491816,
        "median": 0.8366525769233704,
        "min": 0.7952536344528198,
        "max": 0.8760025501251221,
        "count": 13
      },
      "bert_recall": {
        "mean": 0.8468681665567251,
        "std": 0.034034464137079244,
        "median": 0.8393942713737488,
        "min": 0.7925904989242554,
        "max": 0.8951467871665955,
        "count": 13
      },
      "bert_f1": {
        "mean": 0.8404584985512954,
        "std": 0.023039391530251616,
        "median": 0.8380423188209534,
        "min": 0.8054324984550476,
        "max": 0.8829736113548279,
        "count": 13
      },
      "meteor": {
        "mean": 0.03233512535777436,
        "std": 0.05505494082315809,
        "median": 0.0,
        "min": 0.0,
        "max": 0.13888888888888892,
        "count": 13
      },
      "sbert_similarity": {
        "mean": 0.29887947296867,
        "std": 0.14101948619019433,
        "median": 0.28261467814445496,
        "min": 0.04188477620482445,
        "max": 0.5402374863624573,
        "count": 13
      }
    },
    "category_4": {
      "exact_match": {
        "mean": 0.014285714285714285,
        "std": 0.11952286093343938,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 70
      },
      "f1": {
        "mean": 0.26459489914818,
        "std": 0.28135230932203686,
        "median": 0.17424242424242425,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge1_f": {
        "mean": 0.2860120208166238,
        "std": 0.29197113310662065,
        "median": 0.19523809523809524,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge2_f": {
        "mean": 0.17899859699082704,
        "std": 0.287186384236468,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rougeL_f": {
        "mean": 0.27861968683009203,
        "std": 0.29161727594577763,
        "median": 0.16666666666666669,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "bleu1": {
        "mean": 0.20112370468463972,
        "std": 0.23482497700915728,
        "median": 0.11612451095395215,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu2": {
        "mean": 0.14833845316638786,
        "std": 0.2292624544346138,
        "median": 0.029996427770960024,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu3": {
        "mean": 0.12803944660565095,
        "std": 0.21996317071789281,
        "median": 0.02144443358949177,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu4": {
        "mean": 0.10587722368475469,
        "std": 0.20184707924249162,
        "median": 0.018270357606751154,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bert_precision": {
        "mean": 0.8658917273793901,
        "std": 0.046530474208300816,
        "median": 0.8588186800479889,
        "min": 0.772576093673706,
        "max": 0.9998108148574829,
        "count": 70
      },
      "bert_recall": {
        "mean": 0.8800324899809701,
        "std": 0.05274892345854954,
        "median": 0.8750337362289429,
        "min": 0.7209846377372742,
        "max": 0.9998108148574829,
        "count": 70
      },
      "bert_f1": {
        "mean": 0.8723828911781311,
        "std": 0.045320878696127644,
        "median": 0.8652401268482208,
        "min": 0.7921374440193176,
        "max": 0.9998108148574829,
        "count": 70
      },
      "meteor": {
        "mean": 0.2569767365559875,
        "std": 0.3073224713899235,
        "median": 0.14087301587301587,
        "min": 0.0,
        "max": 0.9921875,
        "count": 70
      },
      "sbert_similarity": {
        "mean": 0.44338252962167773,
        "std": 0.2849823998439915,
        "median": 0.38109080493450165,
        "min": 0.016023851931095123,
        "max": 1.0000001192092896,
        "count": 70
      }
    },
    "category_5": {
      "exact_match": {
        "mean": 0.23404255319148937,
        "std": 0.42797632794384316,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 47
      },
      "f1": {
        "mean": 0.26916979557780557,
        "std": 0.4204062942226224,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge1_f": {
        "mean": 0.26831529177553815,
        "std": 0.42055386160781555,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge2_f": {
        "mean": 0.2053191489361702,
        "std": 0.3967577897858865,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rougeL_f": {
        "mean": 0.26831529177553815,
        "std": 0.42055386160781555,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "bleu1": {
        "mean": 0.2551301471729357,
        "std": 0.4213315968508975,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu2": {
        "mean": 0.21817630152791437,
        "std": 0.39254625135319854,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu3": {
        "mean": 0.2100217155338392,
        "std": 0.3925356354873884,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu4": {
        "mean": 0.17722662542936757,
        "std": 0.34756777068551326,
        "median": 0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bert_precision": {
        "mean": 0.899234174413884,
        "std": 0.05732395128263954,
        "median": 0.8754580020904541,
        "min": 0.8406462669372559,
        "max": 1.0000001192092896,
        "count": 47
      },
      "bert_recall": {
        "mean": 0.8804139799260079,
        "std": 0.07401691715740188,
        "median": 0.8506824970245361,
        "min": 0.7694872617721558,
        "max": 1.0000001192092896,
        "count": 47
      },
      "bert_f1": {
        "mean": 0.8893785869821589,
        "std": 0.06422702070403724,
        "median": 0.8650018572807312,
        "min": 0.8076216578483582,
        "max": 1.0000001192092896,
        "count": 47
      },
      "meteor": {
        "mean": 0.2513164557689085,
        "std": 0.40684578529390003,
        "median": 0.0,
        "min": 0.0,
        "max": 0.9993141289437586,
        "count": 47
      },
      "sbert_similarity": {
        "mean": 0.3131776834352616,
        "std": 0.4087777463651963,
        "median": 0.10203901678323746,
        "min": -0.060942042618989944,
        "max": 1.0000001192092896,
        "count": 47
      }
    }
  },
  "individual_results": [
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ support group?",
      "prediction": "12-July-2023",
      "reference": "7 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9035173654556274,
        "bert_recall": 0.939624547958374,
        "bert_f1": 0.9212172627449036,
        "meteor": 0.0,
        "sbert_similarity": 0.7985991835594177
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie paint a sunrise?",
      "prediction": "1:56 pm on 8 May, 2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7459163069725037,
        "bert_recall": 0.8162559270858765,
        "bert_f1": 0.7795025110244751,
        "meteor": 0.0,
        "sbert_similarity": 0.5050359964370728
      }
    },
    {
      "sample_id": 0,
      "question": "What fields would Caroline be likely to pursue in her educaton?",
      "prediction": "career options",
      "reference": "Psychology, counseling certification",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8461048603057861,
        "bert_recall": 0.8304933905601501,
        "bert_f1": 0.8382264375686646,
        "meteor": 0.0,
        "sbert_similarity": 0.3410683870315552
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline research?",
      "prediction": "Cool! What did it look like?",
      "reference": "Adoption agencies",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8090447187423706,
        "bert_recall": 0.8222025036811829,
        "bert_f1": 0.8155704736709595,
        "meteor": 0.0,
        "sbert_similarity": 0.03908083960413933
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's identity?",
      "prediction": "Caroline is a speaker.",
      "reference": "Transgender woman",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8894516825675964,
        "bert_recall": 0.863900363445282,
        "bert_f1": 0.8764898180961609,
        "meteor": 0.0,
        "sbert_similarity": 0.2214636206626892
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie run a charity race?",
      "prediction": "25-May-23",
      "reference": "The sunday before 25 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7994133830070496,
        "bert_recall": 0.7827760577201843,
        "bert_f1": 0.7910072207450867,
        "meteor": 0.0,
        "sbert_similarity": 0.6154395341873169
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie planning on going camping?",
      "prediction": "2 months ago",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7471734285354614,
        "bert_recall": 0.7941582798957825,
        "bert_f1": 0.7699496746063232,
        "meteor": 0.0,
        "sbert_similarity": 0.38597187399864197
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's relationship status?",
      "prediction": "Single",
      "reference": "Single",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline give a speech at a school?",
      "prediction": "6jul23",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8337791562080383,
        "bert_recall": 0.8084808588027954,
        "bert_f1": 0.8209351301193237,
        "meteor": 0.0,
        "sbert_similarity": 0.18743334710597992
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline meet up with her friends, family, and mentors?",
      "prediction": "9-June-2023",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8823566436767578,
        "bert_recall": 0.8532925248146057,
        "bert_f1": 0.867581307888031,
        "meteor": 0.0,
        "sbert_similarity": 0.8266013264656067
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Caroline had her current group of friends for?",
      "prediction": "since-june-2023",
      "reference": "4 years",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7619649171829224,
        "bert_recall": 0.8444073796272278,
        "bert_f1": 0.801070511341095,
        "meteor": 0.0,
        "sbert_similarity": 0.37534937262535095
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Caroline move from 4 years ago?",
      "prediction": "None of the provided conversations mention Caroline moving from anywhere 4 years ago.",
      "reference": "Sweden",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8081661462783813,
        "bert_recall": 0.8234423995018005,
        "bert_f1": 0.8157328367233276,
        "meteor": 0.0,
        "sbert_similarity": 0.09915497153997421
      }
    },
    {
      "sample_id": 0,
      "question": "How long ago was Caroline's 18th birthday?",
      "prediction": "5mos",
      "reference": "10 years ago",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7455346584320068,
        "bert_recall": 0.7522878646850586,
        "bert_f1": 0.748896062374115,
        "meteor": 0.0,
        "sbert_similarity": 0.1496676504611969
      }
    },
    {
      "sample_id": 0,
      "question": "What career path has Caroline decided to persue?",
      "prediction": "Lately, Caroline has been looking into counseling and mental health as a career.",
      "reference": "counseling or mental health for Transgender people",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3,
        "rouge1_f": 0.3,
        "rouge2_f": 0.1111111111111111,
        "rougeL_f": 0.3,
        "bleu1": 0.2,
        "bleu2": 0.11952286093343935,
        "bleu3": 0.04937604958709526,
        "bleu4": 0.030934588294313718,
        "bert_precision": 0.8529120087623596,
        "bert_recall": 0.8440571427345276,
        "bert_f1": 0.8484614491462708,
        "meteor": 0.3362573099415205,
        "sbert_similarity": 0.4584914445877075
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline still want to pursue counseling as a career if she hadn't received support growing up?",
      "prediction": "Yes, based on the context, Caroline would still want to pursue counseling as a career even if she hadn't received support growing up. Her desire stems from her personal experiences and the realization of how important it is for others to have a support system.",
      "reference": "Likely no",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7952536344528198,
        "bert_recall": 0.815875232219696,
        "bert_f1": 0.8054324984550476,
        "meteor": 0.0,
        "sbert_similarity": 0.1361638456583023
      }
    },
    {
      "sample_id": 0,
      "question": "What activities does Melanie partake in?",
      "prediction": "Painting",
      "reference": "pottery, camping, painting, swimming",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.0024787521766663585,
        "bleu2": 0.000783850263336577,
        "bleu3": 0.0005422918887392082,
        "bleu4": 0.0004407913958354948,
        "bert_precision": 0.8332314491271973,
        "bert_recall": 0.7949420809745789,
        "bert_f1": 0.8136365413665771,
        "meteor": 0.0,
        "sbert_similarity": 0.433554470539093
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie sign up for a pottery class?",
      "prediction": "15-jul-23",
      "reference": "2 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8194011449813843,
        "bert_recall": 0.8453108072280884,
        "bert_f1": 0.8321543335914612,
        "meteor": 0.0,
        "sbert_similarity": 0.2921246886253357
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline going to the transgender conference?",
      "prediction": "DATE OF CONVERSATION: 3-July-2023",
      "reference": "July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8169519305229187,
        "bert_recall": 0.8872392177581787,
        "bert_f1": 0.8506461381912231,
        "meteor": 0.0,
        "sbert_similarity": 0.6756954789161682
      }
    },
    {
      "sample_id": 0,
      "question": "Where has Melanie camped?",
      "prediction": "The mountains",
      "reference": "beach, mountains, forest",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.11156508007421491,
        "bleu2": 0.04989342059223037,
        "bleu3": 0.03883455907572571,
        "bleu4": 0.03336574130316504,
        "bert_precision": 0.9250825643539429,
        "bert_recall": 0.8577304482460022,
        "bert_f1": 0.8901342153549194,
        "meteor": 0.0,
        "sbert_similarity": 0.6393359303474426
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's kids like?",
      "prediction": "The kids love nature.",
      "reference": "dinosaurs, nature",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8867316842079163,
        "bert_recall": 0.8530272245407104,
        "bert_f1": 0.8695529103279114,
        "meteor": 0.0,
        "sbert_similarity": 0.44190555810928345
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the museum?",
      "prediction": "8-18-23",
      "reference": "5 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7822551131248474,
        "bert_recall": 0.8178468942642212,
        "bert_f1": 0.7996551990509033,
        "meteor": 0.0,
        "sbert_similarity": 0.26459163427352905
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline have a picnic?",
      "prediction": "8:18 pm on 6 July, 2023",
      "reference": "The week before 6 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8757824301719666,
        "bert_recall": 0.8922140598297119,
        "bert_f1": 0.8839218616485596,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.728199303150177
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline likely have Dr. Seuss books on her bookshelf?",
      "prediction": "false",
      "reference": "Yes, since she collects classic children's books",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8496769666671753,
        "bert_recall": 0.8267220258712769,
        "bert_f1": 0.8380423188209534,
        "meteor": 0.0,
        "sbert_similarity": 0.04188477620482445
      }
    },
    {
      "sample_id": 0,
      "question": "What books has Melanie read?",
      "prediction": "\"Charlotte's Web\"",
      "reference": "\"Nothing is Impossible\", \"Charlotte's Web\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.301194211912202,
        "bleu2": 0.301194211912202,
        "bleu3": 0.301194211912202,
        "bleu4": 0.301194211912202,
        "bert_precision": 0.9526905417442322,
        "bert_recall": 0.914000928401947,
        "bert_f1": 0.9329447746276855,
        "meteor": 0.39893617021276595,
        "sbert_similarity": 0.7511486411094666
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to destress?",
      "prediction": "Painting nature-inspired paintings",
      "reference": "Running, pottery",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8735355138778687,
        "bert_recall": 0.8661095499992371,
        "bert_f1": 0.869806706905365,
        "meteor": 0.0,
        "sbert_similarity": 0.350165992975235
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ conference?",
      "prediction": "2023-07-12",
      "reference": "10 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7812943458557129,
        "bert_recall": 0.8105915784835815,
        "bert_f1": 0.7956733703613281,
        "meteor": 0.0,
        "sbert_similarity": 0.671085000038147
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie read the book \"nothing is impossible\"?",
      "prediction": "10-13-2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7303819060325623,
        "bert_recall": 0.8437559008598328,
        "bert_f1": 0.7829861640930176,
        "meteor": 0.0,
        "sbert_similarity": 0.6538617014884949
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline pursue writing as a career option?",
      "prediction": "Wow, Caroline! What kinda jobs are you thinkin' of? Anything that stands out?",
      "reference": "LIkely no; though she likes reading, she wants to be a counselor",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.05555555555555554,
        "bleu2": 0.018077538151554672,
        "bleu3": 0.013254163846385744,
        "bleu4": 0.010802314890908065,
        "bert_precision": 0.8170138001441956,
        "bert_recall": 0.820177435874939,
        "bert_f1": 0.8185925483703613,
        "meteor": 0.04132231404958677,
        "sbert_similarity": 0.2361619919538498
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the adoption meeting?",
      "prediction": "8_august_2023",
      "reference": "The friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8535175919532776,
        "bert_recall": 0.836548924446106,
        "bert_f1": 0.8449480533599854,
        "meteor": 0.0,
        "sbert_similarity": 0.6665770411491394
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the pottery workshop?",
      "prediction": "15-July-2023",
      "reference": "The Friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8886526823043823,
        "bert_recall": 0.8590015769004822,
        "bert_f1": 0.8735756278038025,
        "meteor": 0.0,
        "sbert_similarity": 0.7687861919403076
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered a member of the LGBTQ community?",
      "prediction": "Yes, based on the context, Melanie would be considered a member of the LGBTQ community as she interacts with and supports the LGBTQ advocacy efforts mentioned.",
      "reference": "Likely no, she does not refer to herself as part of it",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1714285714285714,
        "rouge1_f": 0.15789473684210525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.05263157894736842,
        "bleu1": 0.13793103448275862,
        "bleu2": 0.022194838080923762,
        "bleu3": 0.012769569769300592,
        "bleu4": 0.009152541620698935,
        "bert_precision": 0.8569678068161011,
        "bert_recall": 0.874335765838623,
        "bert_f1": 0.8655646443367004,
        "meteor": 0.1119402985074627,
        "sbert_similarity": 0.5402374863624573
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in June?",
      "prediction": "17-Aug-2023",
      "reference": "The week before 27 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8595733642578125,
        "bert_recall": 0.8373178243637085,
        "bert_f1": 0.8482996225357056,
        "meteor": 0.0,
        "sbert_similarity": 0.6655383110046387
      }
    },
    {
      "sample_id": 0,
      "question": "What LGBTQ+ events has Caroline participated in?",
      "prediction": "LGBTQ+ pride parade, LGBTQ support group, LGBT pride event",
      "reference": "Pride parade, school speech, support group",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5333333333333333,
        "rouge2_f": 0.3076923076923077,
        "rougeL_f": 0.5333333333333333,
        "bleu1": 0.5454545454545454,
        "bleu2": 0.40451991747794525,
        "bleu3": 0.2664896291542936,
        "bleu4": 0.12278260149096115,
        "bert_precision": 0.8583070039749146,
        "bert_recall": 0.9034978747367859,
        "bert_f1": 0.8803228735923767,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.7495311498641968
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to a pride parade during the summer?",
      "prediction": "20jul23",
      "reference": "The week before 3 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8320906758308411,
        "bert_recall": 0.8127378225326538,
        "bert_f1": 0.822300374507904,
        "meteor": 0.0,
        "sbert_similarity": 0.11275410652160645
      }
    },
    {
      "sample_id": 0,
      "question": "What events has Caroline participated in to help children?",
      "prediction": "I totally agree, Caroline. Everyone deserves that. It's awesome to see how passionate you are about helping these kids.",
      "reference": "Mentoring program, school speech",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.041666666666666664,
        "bleu2": 0.013459547551454133,
        "bleu3": 0.009821111002978165,
        "bleu4": 0.007913247271422612,
        "bert_precision": 0.8302880525588989,
        "bert_recall": 0.8110343217849731,
        "bert_f1": 0.8205482959747314,
        "meteor": 0.0,
        "sbert_similarity": 0.3305942118167877
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in July?",
      "prediction": "8:56 pm on 20 July, 2023",
      "reference": "two weekends before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.8467268347740173,
        "bert_recall": 0.8623133897781372,
        "bert_f1": 0.8544490933418274,
        "meteor": 0.08333333333333333,
        "sbert_similarity": 0.5809721350669861
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a mentorship program?",
      "prediction": "17-Jul-23",
      "reference": "The weekend before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8025819659233093,
        "bert_recall": 0.797107458114624,
        "bert_f1": 0.7998352646827698,
        "meteor": 0.0,
        "sbert_similarity": 0.4048652946949005
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie paint recently?",
      "prediction": "[a photo of a field of purple flowers with green leaves]",
      "reference": "sunset",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7812505960464478,
        "bert_recall": 0.8226033449172974,
        "bert_f1": 0.8013938665390015,
        "meteor": 0.0,
        "sbert_similarity": 0.2617217004299164
      }
    },
    {
      "sample_id": 0,
      "question": "What activities has Melanie done with her family?",
      "prediction": "family outings, playing games, hanging out together, beach trips",
      "reference": "Pottery, painting, camping, museum, swimming, hiking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.25,
        "bleu2": 0.047673129462279626,
        "bleu3": 0.02935354896563489,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.844706654548645,
        "bert_recall": 0.8468732833862305,
        "bert_f1": 0.8457885980606079,
        "meteor": 0.0,
        "sbert_similarity": 0.506821870803833
      }
    },
    {
      "sample_id": 0,
      "question": "In what ways is Caroline participating in the LGBTQ community?",
      "prediction": "Joining a new LGBTQ activist group, mentoring a transgender teen, attending LGBTQ meetings, planning events, and supporting others' rights.",
      "reference": "Joining activist group, going to pride parades, participating in an art show, mentoring program",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2580645161290323,
        "rouge1_f": 0.24242424242424243,
        "rouge2_f": 0.06451612903225808,
        "rougeL_f": 0.24242424242424243,
        "bleu1": 0.28,
        "bleu2": 0.1870828693386971,
        "bleu3": 0.11753636863057573,
        "bleu4": 0.051283703648473455,
        "bert_precision": 0.8941839337348938,
        "bert_recall": 0.898068368434906,
        "bert_f1": 0.8961219191551208,
        "meteor": 0.256551724137931,
        "sbert_similarity": 0.7006539702415466
      }
    },
    {
      "sample_id": 0,
      "question": "How many times has Melanie gone to the beach in 2023?",
      "prediction": "2",
      "reference": 2,
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a new activist group?",
      "prediction": "28-Aug-23",
      "reference": "The Tuesday before 20 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7951339483261108,
        "bert_recall": 0.7904825806617737,
        "bert_f1": 0.7928014397621155,
        "meteor": 0.0,
        "sbert_similarity": 0.4891263246536255
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be more interested in going to a national park or a theme park?",
      "prediction": "Based on the given context, there is no mention of Melanie's preferences towards national parks or theme parks.",
      "reference": "National park; she likes the outdoors",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1739130434782609,
        "rouge1_f": 0.23999999999999996,
        "rouge2_f": 0.08695652173913045,
        "rougeL_f": 0.16,
        "bleu1": 0.09523809523809525,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.0141800886003256,
        "bleu4": 0.010862721615727716,
        "bert_precision": 0.8513903617858887,
        "bert_recall": 0.8844992518424988,
        "bert_f1": 0.8676290512084961,
        "meteor": 0.13888888888888892,
        "sbert_similarity": 0.4873815178871155
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of art does Caroline make?",
      "prediction": "painting",
      "reference": "abstract art",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9343193769454956,
        "bert_recall": 0.8527947664260864,
        "bert_f1": 0.8916975855827332,
        "meteor": 0.0,
        "sbert_similarity": 0.5710418224334717
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie's daughter's birthday?",
      "prediction": "20 Jul 2023",
      "reference": "13 August",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8426265120506287,
        "bert_recall": 0.898715615272522,
        "bert_f1": 0.8697677850723267,
        "meteor": 0.0,
        "sbert_similarity": 0.4969671964645386
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline attend a pride parade in August?",
      "prediction": "14th August, 2023",
      "reference": "The Friday before 14 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.07830277146770757,
        "bleu3": 0.05844171443814127,
        "bleu4": 0.05795053470733952,
        "bert_precision": 0.9014451503753662,
        "bert_recall": 0.8778105974197388,
        "bert_f1": 0.8894708752632141,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.8403059840202332
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered an ally to the transgender community?",
      "prediction": "Yes, based on the context, Melanie would be considered an ally to the transgender community given her expressions of appreciation for Caroline's help and motivation, her support for LGBTQ rights, and her engagement with the transgender community through various initiatives and conversations.",
      "reference": "Yes, she is supportive",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.054054054054054064,
        "rouge1_f": 0.0851063829787234,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0851063829787234,
        "bleu1": 0.041666666666666664,
        "bleu2": 0.029774566708770676,
        "bleu3": 0.013002531442793633,
        "bleu4": 0.008089657572395275,
        "bert_precision": 0.8257940411567688,
        "bert_recall": 0.8951467871665955,
        "bert_f1": 0.8590729832649231,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.4624272882938385
      }
    },
    {
      "sample_id": 0,
      "question": "Who supports Caroline when she has a negative experience?",
      "prediction": "Their support system around her, including her family and close friends.",
      "reference": "Her mentors, family, and friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5333333333333333,
        "rouge1_f": 0.5000000000000001,
        "rouge2_f": 0.14285714285714288,
        "rougeL_f": 0.5000000000000001,
        "bleu1": 0.38461538461538464,
        "bleu2": 0.05661385170722978,
        "bleu3": 0.031861733288864436,
        "bleu4": 0.023233421968350106,
        "bert_precision": 0.8957490921020508,
        "bert_recall": 0.9180790781974792,
        "bert_f1": 0.9067766070365906,
        "meteor": 0.1785714285714286,
        "sbert_similarity": 0.6581110954284668
      }
    },
    {
      "sample_id": 0,
      "question": "What types of pottery have Melanie and her kids made?",
      "prediction": "clay sculptures",
      "reference": "bowls, cup",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8815883994102478,
        "bert_recall": 0.8504481315612793,
        "bert_f1": 0.8657382726669312,
        "meteor": 0.0,
        "sbert_similarity": 0.23033252358436584
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline and Melanie go to a pride fesetival together?",
      "prediction": "14 August, 2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7736982107162476,
        "bert_recall": 0.858389139175415,
        "bert_f1": 0.8138463497161865,
        "meteor": 0.0,
        "sbert_similarity": 0.6809490919113159
      }
    },
    {
      "sample_id": 0,
      "question": "What would Caroline's political leaning likely be?",
      "prediction": "Unknown from the provided context",
      "reference": "Liberal",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8010134696960449,
        "bert_recall": 0.8821263313293457,
        "bert_f1": 0.8396154046058655,
        "meteor": 0.0,
        "sbert_similarity": 0.20471303164958954
      }
    },
    {
      "sample_id": 0,
      "question": "What has Melanie painted?",
      "prediction": "a sunset with a palm tree, a sunset with a pink sky, a field of purple flowers with green leaves, a peaceful blue streaks, a painting of a sunset with a palm tree",
      "reference": "Horse, sunset, sunrise",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1,
        "rouge1_f": 0.05555555555555555,
        "rouge2_f": 0.0,
        "rougeL_f": 0.05555555555555555,
        "bleu1": 0.08108108108108107,
        "bleu2": 0.015007505629691604,
        "bleu3": 0.009053597130392502,
        "bleu4": 0.0065958005888164355,
        "bert_precision": 0.8113904595375061,
        "bert_recall": 0.8778421878814697,
        "bert_f1": 0.8433092832565308,
        "meteor": 0.0,
        "sbert_similarity": 0.5102929472923279
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's pets' names?",
      "prediction": "Oscar",
      "reference": "Oliver, Luna, Bailey",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8971912860870361,
        "bert_recall": 0.7953620553016663,
        "bert_f1": 0.8432135581970215,
        "meteor": 0.0,
        "sbert_similarity": 0.19676248729228973
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline apply to adoption agencies?",
      "prediction": "3_august_2023",
      "reference": "The week of 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8577873706817627,
        "bert_recall": 0.8517684936523438,
        "bert_f1": 0.8547673225402832,
        "meteor": 0.0,
        "sbert_similarity": 0.7612043023109436
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline draw a self-portrait?",
      "prediction": "13-Oct-23",
      "reference": "The week before 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7967090606689453,
        "bert_recall": 0.7941392660140991,
        "bert_f1": 0.7954220771789551,
        "meteor": 0.0,
        "sbert_similarity": 0.5457403659820557
      }
    },
    {
      "sample_id": 0,
      "question": "What subject have Caroline and Melanie both painted?",
      "prediction": "painting",
      "reference": "Sunsets",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8751156330108643,
        "bert_recall": 0.8036507368087769,
        "bert_f1": 0.8378620147705078,
        "meteor": 0.0,
        "sbert_similarity": 0.28051090240478516
      }
    },
    {
      "sample_id": 0,
      "question": "What symbols are important to Caroline?",
      "prediction": "The eagle symbolizes freedom and pride, representing resilience.",
      "reference": "Rainbow flag, transgender symbol",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8608695268630981,
        "bert_recall": 0.8461553454399109,
        "bert_f1": 0.8534491062164307,
        "meteor": 0.11363636363636363,
        "sbert_similarity": 0.45853719115257263
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline encounter people on a hike and have a negative experience?",
      "prediction": "3:19 pm on 28 august 2023",
      "reference": "The week before 25 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.8584948778152466,
        "bert_recall": 0.8742824196815491,
        "bert_f1": 0.8663166761398315,
        "meteor": 0.3125,
        "sbert_similarity": 0.7220804691314697
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie make a plate in pottery class?",
      "prediction": "15-jul-23",
      "reference": "24 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8181319236755371,
        "bert_recall": 0.8456268310546875,
        "bert_f1": 0.8316522240638733,
        "meteor": 0.0,
        "sbert_similarity": 0.31528812646865845
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline be considered religious?",
      "prediction": "No",
      "reference": "Somewhat, but not extremely religious",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8270885944366455,
        "bert_recall": 0.7925904989242554,
        "bert_f1": 0.8094721436500549,
        "meteor": 0.0,
        "sbert_similarity": 0.21692019701004028
      }
    },
    {
      "sample_id": 0,
      "question": "What instruments does Melanie play?",
      "prediction": "No instruments.",
      "reference": "clarinet and violin",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8987426161766052,
        "bert_recall": 0.8273718953132629,
        "bert_f1": 0.8615818023681641,
        "meteor": 0.0,
        "sbert_similarity": 0.45639750361442566
      }
    },
    {
      "sample_id": 0,
      "question": "What musical artists/bands has Melanie seen?",
      "prediction": "That information is not explicitly provided in the given context.",
      "reference": "Summer Sounds, Matt Patterson",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8265816569328308,
        "bert_recall": 0.8368065357208252,
        "bert_f1": 0.831662654876709,
        "meteor": 0.0,
        "sbert_similarity": -0.017164062708616257
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the park?",
      "prediction": "8:56 pm on 20 July, 2023",
      "reference": "27 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8394783735275269,
        "bert_recall": 0.8966503143310547,
        "bert_f1": 0.867123007774353,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.7161288857460022
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline's youth center putting on a talent show?",
      "prediction": "28-Aug-23",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7640132308006287,
        "bert_recall": 0.8070056438446045,
        "bert_f1": 0.7849211096763611,
        "meteor": 0.0,
        "sbert_similarity": 0.45442789793014526
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie likely enjoy the song \"The Four Seasons\" by Vivaldi?",
      "prediction": "That's awesome! What type of guitar? Been playing long?",
      "reference": "Yes; it's classical music",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8180794715881348,
        "bert_recall": 0.8411458134651184,
        "bert_f1": 0.8294523358345032,
        "meteor": 0.0,
        "sbert_similarity": 0.28261467814445496
      }
    },
    {
      "sample_id": 0,
      "question": "What are some changes Caroline has faced during her transition journey?",
      "prediction": "Art's exploration and accepting the beauty of imperfections during her transition.",
      "reference": "Changes to her body, losing unsupportive friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8699375987052917,
        "bert_recall": 0.8516457080841064,
        "bert_f1": 0.8606944680213928,
        "meteor": 0.06756756756756757,
        "sbert_similarity": 0.3110599219799042
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do with her family on hikes?",
      "prediction": "Explore the forest and hike.",
      "reference": "Roast marshmallows, tell stories",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8901035785675049,
        "bert_recall": 0.8383480310440063,
        "bert_f1": 0.8634509444236755,
        "meteor": 0.0,
        "sbert_similarity": 0.17731456458568573
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go biking with friends?",
      "prediction": "13-09-2023",
      "reference": "The weekend before 13 September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8217763900756836,
        "bert_recall": 0.8131442070007324,
        "bert_f1": 0.8174375295639038,
        "meteor": 0.0,
        "sbert_similarity": 0.6330372095108032
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been practicing art?",
      "prediction": "7 years",
      "reference": "Since 2016",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9006768465042114,
        "bert_recall": 0.9006768465042114,
        "bert_f1": 0.9006768465042114,
        "meteor": 0.0,
        "sbert_similarity": 0.4524037539958954
      }
    },
    {
      "sample_id": 0,
      "question": "What personality traits might Melanie say Caroline has?",
      "prediction": "caring heart",
      "reference": "Thoughtful, authentic, driven",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8517429828643799,
        "bert_recall": 0.8167228102684021,
        "bert_f1": 0.8338653445243835,
        "meteor": 0.0,
        "sbert_similarity": 0.3124227523803711
      }
    },
    {
      "sample_id": 0,
      "question": "What transgender-specific events has Caroline attended?",
      "prediction": "LGBTQ pride event, transgender conference",
      "reference": "Poetry reading, conference",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.08164965809277261,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.048549177170732344,
        "bert_precision": 0.8515130877494812,
        "bert_recall": 0.8780134916305542,
        "bert_f1": 0.8645602464675903,
        "meteor": 0.15625,
        "sbert_similarity": 0.27329790592193604
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Melanie read from Caroline's suggestion?",
      "prediction": "That book I read last year",
      "reference": "\"Becoming Nicole\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8115492463111877,
        "bert_recall": 0.8239882588386536,
        "bert_f1": 0.8177214860916138,
        "meteor": 0.0,
        "sbert_similarity": 0.15688647329807281
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's friend adopt a child?",
      "prediction": "10oct2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7965106964111328,
        "bert_recall": 0.9024755358695984,
        "bert_f1": 0.8461886048316956,
        "meteor": 0.0,
        "sbert_similarity": 0.2384667694568634
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie get hurt?",
      "prediction": "17 July",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8821901082992554,
        "bert_recall": 0.8494344353675842,
        "bert_f1": 0.8655024170875549,
        "meteor": 0.0,
        "sbert_similarity": 0.5464493632316589
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's family go on a roadtrip?",
      "prediction": "20th October, 2023",
      "reference": "The weekend before 20 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.07830277146770757,
        "bleu3": 0.05844171443814127,
        "bleu4": 0.05795053470733952,
        "bert_precision": 0.8946911096572876,
        "bert_recall": 0.8740978240966797,
        "bert_f1": 0.8842746019363403,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.8029320240020752
      }
    },
    {
      "sample_id": 0,
      "question": "How many children does Melanie have?",
      "prediction": "[]",
      "reference": 3,
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8903661966323853,
        "bert_recall": 0.8903661966323853,
        "bert_f1": 0.8903661966323853,
        "meteor": 0.0,
        "sbert_similarity": 0.2892223298549652
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go on a hike after the roadtrip?",
      "prediction": "That sounds great, Caroline. It's awesome having stuff around that make us think of good connections and times. Actually, I just took my fam camping in the mountains last week - it was a really nice time together!",
      "reference": "19 October 2023",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7965023517608643,
        "bert_recall": 0.7920097708702087,
        "bert_f1": 0.7942496538162231,
        "meteor": 0.0,
        "sbert_similarity": -0.05635596439242363
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie go on another roadtrip soon?",
      "prediction": "No, based on the provided context, Melanie would not be planning another road trip soon. The context mentions multiple talks but none discussing plans for a road trip.",
      "reference": "Likely no; since this one went badly",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.05714285714285715,
        "rouge2_f": 0.0,
        "rougeL_f": 0.05714285714285715,
        "bleu1": 0.03125,
        "bleu2": 0.010040241611281232,
        "bleu3": 0.007306361129019465,
        "bleu4": 0.005834347180338519,
        "bert_precision": 0.8366525769233704,
        "bert_recall": 0.8393942713737488,
        "bert_f1": 0.8380211591720581,
        "meteor": 0.0,
        "sbert_similarity": 0.2698556184768677
      }
    },
    {
      "sample_id": 0,
      "question": "What items has Melanie bought?",
      "prediction": "[a photo of a row of white chairs with flowers on them], [a photography of a family standing on the beach at sunset], [a photo of a person wearing pink sneakers on a white rug]",
      "reference": "Figurines, shoes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.023255813953488372,
        "bleu2": 0.007441168295606795,
        "bleu3": 0.005408346040006841,
        "bleu4": 0.004286566854494811,
        "bert_precision": 0.796971321105957,
        "bert_recall": 0.7687082290649414,
        "bert_f1": 0.7825846672058105,
        "meteor": 0.0,
        "sbert_similarity": 0.44632866978645325
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline pass the adoption interview?",
      "prediction": "9-22-23",
      "reference": "The Friday before 22 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7951096296310425,
        "bert_recall": 0.7919362187385559,
        "bert_f1": 0.7935197353363037,
        "meteor": 0.0,
        "sbert_similarity": 0.365335613489151
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie buy the figurines?",
      "prediction": "DATE OF CONVERSATION 22-10-2023",
      "reference": "21 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8439952731132507,
        "bert_recall": 0.9107730388641357,
        "bert_f1": 0.8761135339736938,
        "meteor": 0.0,
        "sbert_similarity": 0.5914703011512756
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline want to move back to her home country soon?",
      "prediction": "No, Caroline does not express a desire to move back to her home country soon.",
      "reference": "No; she's in the process of adopting children.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.08333333333333334,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08333333333333334,
        "bleu1": 0.11764705882352941,
        "bleu2": 0.027116307227332027,
        "bleu3": 0.017693837921724947,
        "bleu4": 0.013679192123121896,
        "bert_precision": 0.8760025501251221,
        "bert_recall": 0.8900565505027771,
        "bert_f1": 0.8829736113548279,
        "meteor": 0.0,
        "sbert_similarity": 0.35358157753944397
      }
    },
    {
      "sample_id": 0,
      "question": "What did the charity race raise awareness for?",
      "prediction": "The charity race raised awareness for mental health.",
      "reference": "mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.16666666666666669,
        "bleu3": 0.07542890820012248,
        "bleu4": 0.050712153369465586,
        "bert_precision": 0.8395984768867493,
        "bert_recall": 0.866235613822937,
        "bert_f1": 0.8527090549468994,
        "meteor": 0.19230769230769235,
        "sbert_similarity": 0.6050230264663696
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie realize after the charity race?",
      "prediction": "it was really rewarding. Really made me think about taking care of our minds.",
      "reference": "self-care is important",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.11111111111111112,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11111111111111112,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8679269552230835,
        "bert_recall": 0.8922924995422363,
        "bert_f1": 0.8799410462379456,
        "meteor": 0.0,
        "sbert_similarity": 0.23238076269626617
      }
    },
    {
      "sample_id": 0,
      "question": "How does Melanie prioritize self-care?",
      "prediction": "Taking care of myself is so important - even if it's not always easy.",
      "reference": "by carving out some me-time each day for activities like running, reading, or playing the violin",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.842543363571167,
        "bert_recall": 0.8513495326042175,
        "bert_f1": 0.8469235301017761,
        "meteor": 0.03164556962025316,
        "sbert_similarity": 0.2922074794769287
      }
    },
    {
      "sample_id": 0,
      "question": "What are Caroline's plans for the summer?",
      "prediction": "Right, it was so much fun! We could do a family outing, or wanna plan something special for this summer, just us two?",
      "reference": "researching adoption agencies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.801048755645752,
        "bert_recall": 0.8370177149772644,
        "bert_f1": 0.818638265132904,
        "meteor": 0.0,
        "sbert_similarity": 0.016023851931095123
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Caroline is considering support?",
      "prediction": "Kids who need it",
      "reference": "LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8358721733093262,
        "bert_recall": 0.8277864456176758,
        "bert_f1": 0.8318096995353699,
        "meteor": 0.0,
        "sbert_similarity": 0.21965277194976807
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose the adoption agency?",
      "prediction": "To turn her dream of having a family and providing a loving home for kids into reality.",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.24000000000000005,
        "rouge1_f": 0.23076923076923078,
        "rouge2_f": 0.0,
        "rougeL_f": 0.23076923076923078,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.03131121455425748,
        "bleu3": 0.019045937245652737,
        "bleu4": 0.014216645907653848,
        "bert_precision": 0.8438078761100769,
        "bert_recall": 0.8231152296066284,
        "bert_f1": 0.8333331346511841,
        "meteor": 0.1530612244897959,
        "sbert_similarity": 0.18581397831439972
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline excited about in the adoption process?",
      "prediction": "Gathering documents like references, financial info and medical checks; preparing emotionally; turning her dream into a reality; researching adoption agencies; finding an adoption agency or lawyer; it's all worth it in the end",
      "reference": "creating a family for kids who need one",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.05,
        "rouge1_f": 0.047619047619047616,
        "rouge2_f": 0.0,
        "rougeL_f": 0.047619047619047616,
        "bleu1": 0.025,
        "bleu2": 0.008006407690254359,
        "bleu3": 0.005820231990246252,
        "bleu4": 0.004620856909230222,
        "bert_precision": 0.8005560636520386,
        "bert_recall": 0.8226131796836853,
        "bert_f1": 0.811434805393219,
        "meteor": 0.04761904761904762,
        "sbert_similarity": 0.3819536566734314
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie think about Caroline's decision to adopt?",
      "prediction": "That's great, Caroline! Loving the inclusivity and support. Anything you're excited for in the adoption process?",
      "reference": "she thinks Caroline is doing something amazing and will be an awesome mom",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.12903225806451615,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12903225806451615,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.020806259464411975,
        "bleu3": 0.013510398157576722,
        "bleu4": 0.010331208012220438,
        "bert_precision": 0.8591680526733398,
        "bert_recall": 0.8686308860778809,
        "bert_f1": 0.8638736009597778,
        "meteor": 0.03759398496240602,
        "sbert_similarity": 0.559855580329895
      }
    },
    {
      "sample_id": 0,
      "question": "How long have Mel and her husband been married?",
      "prediction": "Wow, what an amazing family pic! How long have you been married?",
      "reference": "Mel and her husband have been married for 5 years.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2727272727272727,
        "rouge1_f": 0.2727272727272727,
        "rouge2_f": 0.09999999999999999,
        "rougeL_f": 0.2727272727272727,
        "bleu1": 0.2,
        "bleu2": 0.11952286093343935,
        "bleu3": 0.04937604958709526,
        "bleu4": 0.030934588294313718,
        "bert_precision": 0.8636370897293091,
        "bert_recall": 0.8866586685180664,
        "bert_f1": 0.8749964833259583,
        "meteor": 0.09803921568627452,
        "sbert_similarity": 0.32091647386550903
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's necklace symbolize?",
      "prediction": "Love, faith, and strength",
      "reference": "love, faith, and strength",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9998108148574829,
        "bert_recall": 0.9998108148574829,
        "bert_f1": 0.9998108148574829,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Caroline's grandma from?",
      "prediction": "{}\n{\"answer\":\"US\"}",
      "reference": "Sweden",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.772576093673706,
        "bert_recall": 0.9073703289031982,
        "bert_f1": 0.8345654606819153,
        "meteor": 0.0,
        "sbert_similarity": 0.21224024891853333
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Caroline?",
      "prediction": "A necklace",
      "reference": "necklace",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.8517659902572632,
        "bert_recall": 0.9151227474212646,
        "bert_f1": 0.8823084831237793,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.9594278931617737
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's hand-painted bowl a reminder of?",
      "prediction": "Creating art was really important to me during my transition - it helped me understand and accept myself.",
      "reference": "art and self-expression",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.19047619047619047,
        "rouge2_f": 0.0,
        "rougeL_f": 0.19047619047619047,
        "bleu1": 0.10526315789473682,
        "bleu2": 0.024182541670333724,
        "bleu3": 0.015742117131956883,
        "bleu4": 0.012109013026441871,
        "bert_precision": 0.8480849862098694,
        "bert_recall": 0.8799780607223511,
        "bert_f1": 0.863737165927887,
        "meteor": 0.22222222222222227,
        "sbert_similarity": 0.5742111802101135
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family do while camping?",
      "prediction": "Exploring the forest and hiking.",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.42857142857142855,
        "rouge2_f": 0.0,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.07243303475117972,
        "bleu2": 0.025091539267089076,
        "bleu3": 0.019586557892750135,
        "bleu4": 0.01774239756616722,
        "bert_precision": 0.8914598226547241,
        "bert_recall": 0.8742300868034363,
        "bert_f1": 0.8827608823776245,
        "meteor": 0.11627906976744186,
        "sbert_similarity": 0.624316394329071
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Caroline interested in pursuing?",
      "prediction": "Counseling and mental health services.",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.125,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.13179856905786336,
        "bleu2": 0.08335673411476816,
        "bleu3": 0.03649731313368611,
        "bleu4": 0.025185198565837008,
        "bert_precision": 0.8726141452789307,
        "bert_recall": 0.8471193909645081,
        "bert_f1": 0.859677791595459,
        "meteor": 0.20947176684881602,
        "sbert_similarity": 0.5427908301353455
      }
    },
    {
      "sample_id": 0,
      "question": "What workshop did Caroline attend recently?",
      "prediction": "No relevant workshop information is provided in the given context.",
      "reference": "LGBTQ+ counseling workshop",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8432290554046631,
        "bert_recall": 0.8348435163497925,
        "bert_f1": 0.8390153050422668,
        "meteor": 0.13513513513513514,
        "sbert_similarity": 0.2777039408683777
      }
    },
    {
      "sample_id": 0,
      "question": "What was discussed in the LGBTQ+ counseling workshop?",
      "prediction": "Different therapeutic methods and how to best work with trans people.",
      "reference": "therapeutic methods and how to best work with trans people",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9523809523809523,
        "rouge1_f": 0.9523809523809523,
        "rouge2_f": 0.9473684210526316,
        "rougeL_f": 0.9523809523809523,
        "bleu1": 0.8333333333333334,
        "bleu2": 0.8257228238447705,
        "bleu3": 0.8187104363805253,
        "bleu4": 0.8070557274927982,
        "bert_precision": 0.9778479337692261,
        "bert_recall": 0.9872344136238098,
        "bert_f1": 0.98251873254776,
        "meteor": 0.8904779366825571,
        "sbert_similarity": 0.9825106859207153
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Caroline to pursue counseling?",
      "prediction": "Struggling with mental health, support received was really helpful, which made her realize how important it is for others to have a support system. So, she started looking into counseling and mental health career options to help other people.",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2978723404255319,
        "rouge1_f": 0.2641509433962264,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1509433962264151,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.020562903028816627,
        "bleu3": 0.01049454118628617,
        "bleu4": 0.007039372171487214,
        "bert_precision": 0.8566441535949707,
        "bert_recall": 0.8820974826812744,
        "bert_f1": 0.8691845536231995,
        "meteor": 0.21212121212121213,
        "sbert_similarity": 0.7515699863433838
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Caroline want to create for people?",
      "prediction": "community",
      "reference": "a safe and inviting place for people to grow",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8522070050239563,
        "bert_recall": 0.7973489761352539,
        "bert_f1": 0.8238658308982849,
        "meteor": 0.0,
        "sbert_similarity": 0.32440412044525146
      }
    },
    {
      "sample_id": 0,
      "question": "Did Melanie make the black and white bowl in the photo?",
      "prediction": "Yes, Melanie made the black and white bowl in the photo.",
      "reference": "Yes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8196587562561035,
        "bert_recall": 0.8515076637268066,
        "bert_f1": 0.8352797031402588,
        "meteor": 0.0,
        "sbert_similarity": 0.18057873845100403
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of books does Caroline have in her library?",
      "prediction": "Caroline has kids' books - classics, stories from different cultures, educational books.",
      "reference": "kids' books - classics, stories from different cultures, educational books",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9,
        "rouge1_f": 0.9,
        "rouge2_f": 0.888888888888889,
        "rougeL_f": 0.9,
        "bleu1": 0.8125,
        "bleu2": 0.806225774829855,
        "bleu3": 0.801122154487294,
        "bleu4": 0.7916963878457504,
        "bert_precision": 0.9494441747665405,
        "bert_recall": 0.9572364091873169,
        "bert_f1": 0.9533243179321289,
        "meteor": 0.866013071895425,
        "sbert_similarity": 0.7148312926292419
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's favorite book from her childhood?",
      "prediction": "\"I loved reading 'Charlotte's Web' as a kid.\"",
      "reference": "\"Charlotte's Web\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.30769230769230765,
        "bleu2": 0.16012815380508713,
        "bleu3": 0.0632832964415753,
        "bleu4": 0.039073802494525016,
        "bert_precision": 0.8953226804733276,
        "bert_recall": 0.9483333230018616,
        "bert_f1": 0.9210658669471741,
        "meteor": 0.0,
        "sbert_similarity": 0.7759613394737244
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Caroline recommend to Melanie?",
      "prediction": "\"That book you recommended a while ago\"",
      "reference": "\"Becoming Nicole\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.05270462766947299,
        "bleu3": 0.03528075103412339,
        "bleu4": 0.028517539529041507,
        "bert_precision": 0.8388442993164062,
        "bert_recall": 0.8650026321411133,
        "bert_f1": 0.8517226576805115,
        "meteor": 0.0,
        "sbert_similarity": 0.14059798419475555
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline take away from the book \"Becoming Nicole\"?",
      "prediction": "It made her feel connected and gave her a lot of hope for her own path.",
      "reference": "Lessons on self-acceptance and finding support",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.8650250434875488,
        "bert_recall": 0.861440896987915,
        "bert_f1": 0.8632292747497559,
        "meteor": 0.14285714285714288,
        "sbert_similarity": 0.23448166251182556
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Melanie got used for?",
      "prediction": "new shoes used for a photo shoot or as casual wear, as no specific purpose is mentioned.",
      "reference": "Running",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.799496054649353,
        "bert_recall": 0.8311750888824463,
        "bert_f1": 0.8150278925895691,
        "meteor": 0.0,
        "sbert_similarity": 0.1271747648715973
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's reason for getting into running?",
      "prediction": "Running can really boost your mood.",
      "reference": "To de-stress and clear her mind",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8941391706466675,
        "bert_recall": 0.8921612501144409,
        "bert_f1": 0.8931491374969482,
        "meteor": 0.0,
        "sbert_similarity": 0.2972654402256012
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie say running has been great for?",
      "prediction": "running has been great for her headspace.",
      "reference": "Her mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8505576848983765,
        "bert_recall": 0.8503251075744629,
        "bert_f1": 0.8504413962364197,
        "meteor": 0.14705882352941177,
        "sbert_similarity": 0.3802279531955719
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids make during the pottery workshop?",
      "prediction": "Mel and her kids made clay pots during the pottery workshop.",
      "reference": "pots",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666669,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8095608949661255,
        "bert_recall": 0.8474521636962891,
        "bert_f1": 0.828073263168335,
        "meteor": 0.25000000000000006,
        "sbert_similarity": 0.46856409311294556
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of pot did Mel and her kids make with clay?",
      "prediction": "A pot.",
      "reference": "a cup with a dog face on it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.06295853427918728,
        "bleu2": 0.024383735476492117,
        "bleu3": 0.022876308829024167,
        "bleu4": 0.021460411779726157,
        "bert_precision": 0.9361442923545837,
        "bert_recall": 0.8549909591674805,
        "bert_f1": 0.8937290906906128,
        "meteor": 0.06756756756756756,
        "sbert_similarity": 0.33722156286239624
      }
    },
    {
      "sample_id": 0,
      "question": "What creative project do Mel and her kids do together besides pottery?",
      "prediction": "Making clay sculptures",
      "reference": "painting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8188171982765198,
        "bert_recall": 0.9068286418914795,
        "bert_f1": 0.8605785965919495,
        "meteor": 0.0,
        "sbert_similarity": 0.41896113753318787
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids paint in their latest project in July 2023?",
      "prediction": "[a painting of a sunset with a palm tree]",
      "reference": "a sunset with a palm tree",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6153846153846154,
        "rouge1_f": 0.8,
        "rouge2_f": 0.7692307692307693,
        "rougeL_f": 0.8,
        "bleu1": 0.5454545454545454,
        "bleu2": 0.5222329678670935,
        "bleu3": 0.49839100664976926,
        "bleu4": 0.4617366309441026,
        "bert_precision": 0.9016937017440796,
        "bert_recall": 0.948536217212677,
        "bert_f1": 0.9245219826698303,
        "meteor": 0.7904761904761907,
        "sbert_similarity": 0.8051198124885559
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline see at the council meeting for adoption?",
      "prediction": "That she saw inspiring and emotional scenes as many people wanted to create loving homes for children in need.",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.7333333333333334,
        "rouge2_f": 0.7142857142857143,
        "rougeL_f": 0.7333333333333334,
        "bleu1": 0.5,
        "bleu2": 0.4588314677411235,
        "bleu3": 0.4161415637155136,
        "bleu4": 0.37903259135181483,
        "bert_precision": 0.9051300883293152,
        "bert_recall": 0.9505944848060608,
        "bert_f1": 0.9273053407669067,
        "meteor": 0.8470338983050848,
        "sbert_similarity": 0.6498810648918152
      }
    },
    {
      "sample_id": 0,
      "question": "What do sunflowers represent according to Caroline?",
      "prediction": "Blue's my fave, it makes me feel relaxed. Sunflowers mean warmth and happiness.",
      "reference": "warmth and happiness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.375,
        "rouge1_f": 0.35294117647058826,
        "rouge2_f": 0.2666666666666667,
        "rougeL_f": 0.35294117647058826,
        "bleu1": 0.17647058823529413,
        "bleu2": 0.14852213144650114,
        "bleu3": 0.11621764179681902,
        "bleu4": 0.05692994482485313,
        "bert_precision": 0.8393304347991943,
        "bert_recall": 0.8769490718841553,
        "bert_f1": 0.8577275276184082,
        "meteor": 0.46875,
        "sbert_similarity": 0.36088690161705017
      }
    },
    {
      "sample_id": 0,
      "question": "Why are flowers important to Melanie?",
      "prediction": "Flowers bring joy, represent growth, beauty, and remind her to appreciate the small moments; they were an important part of her wedding decor.",
      "reference": "They remind her to appreciate the small moments and were a part of her wedding decor",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7027027027027029,
        "rouge1_f": 0.7692307692307693,
        "rouge2_f": 0.5405405405405405,
        "rougeL_f": 0.6666666666666667,
        "bleu1": 0.5357142857142857,
        "bleu2": 0.44543540318737396,
        "bleu3": 0.39744442901529925,
        "bleu4": 0.34791594751284466,
        "bert_precision": 0.8998556733131409,
        "bert_recall": 0.9328704476356506,
        "bert_f1": 0.9160656929016113,
        "meteor": 0.8053281192716607,
        "sbert_similarity": 0.7465839982032776
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's painting for the art show?",
      "prediction": "The togetherness and power shown in the painting.",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.08897082254631197,
        "bleu2": 0.02984172108414368,
        "bleu3": 0.022474396477098004,
        "bleu4": 0.019201920550239778,
        "bert_precision": 0.8877920508384705,
        "bert_recall": 0.8515603542327881,
        "bert_f1": 0.8692988157272339,
        "meteor": 0.04672897196261682,
        "sbert_similarity": 0.18483059108257294
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Melanie go to the beach with her kids?",
      "prediction": "We don't go often, usually only once or twice a year.",
      "reference": "once or twice a year",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.625,
        "rouge1_f": 0.5882352941176471,
        "rouge2_f": 0.5333333333333333,
        "rougeL_f": 0.5882352941176471,
        "bleu1": 0.3571428571428572,
        "bleu2": 0.33149677206589795,
        "bleu3": 0.3053771540532653,
        "bleu4": 0.26584835766658776,
        "bert_precision": 0.8711920976638794,
        "bert_recall": 0.9022736549377441,
        "bert_f1": 0.8864604830741882,
        "meteor": 0.7087053571428573,
        "sbert_similarity": 0.5874711275100708
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family see during their camping trip last year?",
      "prediction": "The Perseid meteor shower",
      "reference": "Perseid meteor shower",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.75,
        "bleu2": 0.7071067811865475,
        "bleu3": 0.63287829698514,
        "bleu4": 0.3976353643835253,
        "bert_precision": 0.909468412399292,
        "bert_recall": 0.9297232627868652,
        "bert_f1": 0.9194843173027039,
        "meteor": 0.9498207885304659,
        "sbert_similarity": 0.9688671231269836
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel while watching the meteor shower?",
      "prediction": "The sky was so clear and filled with stars, and the meteor shower was amazing - it felt like we were part of something huge and awe-inspiring.",
      "reference": "in awe of the universe",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.18750000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0625,
        "bleu1": 0.06896551724137931,
        "bleu2": 0.01569412051435861,
        "bleu3": 0.0101586586333325,
        "bleu4": 0.007696339439306161,
        "bert_precision": 0.8289888501167297,
        "bert_recall": 0.8751678466796875,
        "bert_f1": 0.8514527082443237,
        "meteor": 0.1388888888888889,
        "sbert_similarity": 0.40577152371406555
      }
    },
    {
      "sample_id": 0,
      "question": "Whose birthday did Melanie celebrate recently?",
      "prediction": "Melanie's daughter's birthday",
      "reference": "Melanie's daughter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.6,
        "bleu2": 0.5477225575051662,
        "bleu3": 0.4677351412871982,
        "bleu4": 0.26591479484724945,
        "bert_precision": 0.9151952266693115,
        "bert_recall": 0.9586464166641235,
        "bert_f1": 0.9364170432090759,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8563249111175537
      }
    },
    {
      "sample_id": 0,
      "question": "Who performed at the concert at Melanie's daughter's birthday?",
      "prediction": "The context does not specify who performed at the concert.",
      "reference": "Matt Patterson",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8391083478927612,
        "bert_recall": 0.812487006187439,
        "bert_f1": 0.8255831003189087,
        "meteor": 0.0,
        "sbert_similarity": 0.06739755719900131
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose to use colors and patterns in her pottery project?",
      "prediction": "The context does not explicitly state why Melanie chose to use colors and patterns in her pottery project.",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.21428571428571427,
        "rouge1_f": 0.21428571428571427,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.21052631578947367,
        "bleu2": 0.03419927840283848,
        "bleu3": 0.019788051778159007,
        "bleu4": 0.014400124446705301,
        "bert_precision": 0.8543208837509155,
        "bert_recall": 0.8912185430526733,
        "bert_f1": 0.8723797798156738,
        "meteor": 0.1388888888888889,
        "sbert_similarity": 0.2799544930458069
      }
    },
    {
      "sample_id": 0,
      "question": "What pet does Caroline have?",
      "prediction": "None of the conversations mention Caroline having a pet.",
      "reference": "guinea pig",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8421391844749451,
        "bert_recall": 0.7796850800514221,
        "bert_f1": 0.8097096085548401,
        "meteor": 0.0,
        "sbert_similarity": 0.14374589920043945
      }
    },
    {
      "sample_id": 0,
      "question": "What pets does Melanie have?",
      "prediction": "[Image: a photo of a cat laying on the floor with its head on the floor], [Image: a photo of a field of purple flowers with green leaves] - Melanie has a cat and a kitty (presumably a cat).",
      "reference": "Two cats and a dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.1395348837209302,
        "rouge2_f": 0.0975609756097561,
        "rougeL_f": 0.1395348837209302,
        "bleu1": 0.040816326530612235,
        "bleu2": 0.029160592175990215,
        "bleu3": 0.012734247624506869,
        "bleu4": 0.007919253535534795,
        "bert_precision": 0.8001905679702759,
        "bert_recall": 0.8727954626083374,
        "bert_f1": 0.8349175453186035,
        "meteor": 0.30423280423280435,
        "sbert_similarity": 0.5978677272796631
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oliver hide his bone once?",
      "prediction": "Oliver hid his bone in Melanie's slipper once.",
      "reference": "In Melanie's slipper",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.6153846153846153,
        "rouge2_f": 0.5454545454545454,
        "rougeL_f": 0.6153846153846153,
        "bleu1": 0.4,
        "bleu2": 0.36514837167011077,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.2626909894424158,
        "bert_precision": 0.8904646635055542,
        "bert_recall": 0.9110816717147827,
        "bert_f1": 0.900655210018158,
        "meteor": 0.8412698412698413,
        "sbert_similarity": 0.6000194549560547
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Caroline used to do with her dad?",
      "prediction": "None of the provided context mentions Caroline doing an activity with her dad.",
      "reference": "Horseback riding",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8266366124153137,
        "bert_recall": 0.760402500629425,
        "bert_f1": 0.7921374440193176,
        "meteor": 0.0,
        "sbert_similarity": 0.039141472429037094
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline make for a local church?",
      "prediction": "[Image: a photo of three stained glass windows in a church with a clock]",
      "reference": "a stained glass window",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.375,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.17647058823529413,
        "bleu2": 0.10502100630210073,
        "bleu3": 0.043244627469917335,
        "bleu4": 0.026920508809559315,
        "bert_precision": 0.8658503293991089,
        "bert_recall": 0.9330596327781677,
        "bert_f1": 0.8981994986534119,
        "meteor": 0.75,
        "sbert_similarity": 0.712457537651062
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline find in her neighborhood during her walk?",
      "prediction": "No information provided about Caroline finding something in her neighborhood during her walk.",
      "reference": "a rainbow sidewalk",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8479675054550171,
        "bert_recall": 0.8633630275726318,
        "bert_f1": 0.8555960059165955,
        "meteor": 0.0,
        "sbert_similarity": 0.10068148374557495
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Caroline to be courageous?",
      "prediction": "\"Cool\" by Sara Bareilles",
      "reference": "Brave by Sara Bareilles",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.75,
        "rouge1_f": 0.75,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.75,
        "bleu1": 0.5,
        "bleu2": 0.447213595499958,
        "bleu3": 0.3721003696282278,
        "bleu4": 0.20205155046766235,
        "bert_precision": 0.9051188826560974,
        "bert_recall": 0.9463179111480713,
        "bert_f1": 0.9252599477767944,
        "meteor": 0.7361111111111112,
        "sbert_similarity": 0.7213602662086487
      }
    },
    {
      "sample_id": 0,
      "question": "Which  classical musicians does Melanie enjoy listening to?",
      "prediction": "[\"Melaniesays : I'm a fan of both classical like Bach and Mozart\"]",
      "reference": "Bach and Mozart",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.4,
        "rouge2_f": 0.3076923076923077,
        "rougeL_f": 0.4,
        "bleu1": 0.17647058823529413,
        "bleu2": 0.14852213144650114,
        "bleu3": 0.11621764179681902,
        "bleu4": 0.05692994482485313,
        "bert_precision": 0.8181168437004089,
        "bert_recall": 0.8910075426101685,
        "bert_f1": 0.8530079126358032,
        "meteor": 0.48076923076923084,
        "sbert_similarity": 0.7207745909690857
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Melanie a fan of in terms of modern music?",
      "prediction": "Caroline",
      "reference": "Ed Sheeran",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8876967430114746,
        "bert_recall": 0.7209846377372742,
        "bert_f1": 0.7957022786140442,
        "meteor": 0.0,
        "sbert_similarity": 0.18620462715625763
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been creating art?",
      "prediction": "Seven years.",
      "reference": "7 years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9816300868988037,
        "bert_recall": 0.9724689722061157,
        "bert_f1": 0.9770280718803406,
        "meteor": 0.25,
        "sbert_similarity": 0.9316551685333252
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Melanie see at the caf\u00e9?",
      "prediction": "The precautionary sign Melanie saw at the caf\u00e9 was just a sign stating that someone is not being able to leave.",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7333333333333334,
        "rouge1_f": 0.6875000000000001,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.6875000000000001,
        "bleu1": 0.5,
        "bleu2": 0.4879500364742666,
        "bleu3": 0.47850652438804997,
        "bleu4": 0.4608662469973654,
        "bert_precision": 0.9065433144569397,
        "bert_recall": 0.9455735683441162,
        "bert_f1": 0.9256472587585449,
        "meteor": 0.8329166666666666,
        "sbert_similarity": 0.6546800136566162
      }
    },
    {
      "sample_id": 0,
      "question": "What advice does Caroline give for getting started with adoption?",
      "prediction": "Do your research and find an adoption agency or lawyer. They'll help with the process and provide all the info. Gather documents like references, financial info and medical checks. Don't forget to prepare emotionally, since the wait can be hard.",
      "reference": "Do research, find an adoption agency or lawyer, gather necessary documents, and prepare emotionally.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5306122448979592,
        "rouge1_f": 0.46428571428571436,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.46428571428571436,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.2062842492517587,
        "bleu3": 0.15758588611896324,
        "bleu4": 0.12532443621763048,
        "bert_precision": 0.8881880044937134,
        "bert_recall": 0.9453722238540649,
        "bert_f1": 0.9158884286880493,
        "meteor": 0.49568644950171065,
        "sbert_similarity": 0.8916807174682617
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Melanie face in October 2023?",
      "prediction": "There is no mention of a setback faced by Melanie in October 2023 in the provided context.",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07407407407407407,
        "rouge1_f": 0.07142857142857142,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07142857142857142,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.02556549962824567,
        "bleu3": 0.016660661223010558,
        "bleu4": 0.01284618972676772,
        "bert_precision": 0.8584693074226379,
        "bert_recall": 0.8748996257781982,
        "bert_f1": 0.8666066527366638,
        "meteor": 0.04310344827586207,
        "sbert_similarity": 0.24234089255332947
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to keep herself busy during her pottery break?",
      "prediction": "Melanie took her kids to a pottery workshop.",
      "reference": "Read a book and paint.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.05270462766947299,
        "bleu3": 0.03528075103412339,
        "bleu4": 0.028517539529041507,
        "bert_precision": 0.8711822628974915,
        "bert_recall": 0.8772447109222412,
        "bert_f1": 0.8742030262947083,
        "meteor": 0.18867924528301888,
        "sbert_similarity": 0.14638501405715942
      }
    },
    {
      "sample_id": 0,
      "question": "What painting did Melanie show to Caroline on October 13, 2023?",
      "prediction": "a photo of a painting of a sunset with a pink sky",
      "reference": "A painting inspired by sunsets with a pink sky.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.625,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5263157894736842,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.4264014327112209,
        "bleu3": 0.33498102801830926,
        "bleu4": 0.2521193618434983,
        "bert_precision": 0.9354541301727295,
        "bert_recall": 0.9433470964431763,
        "bert_f1": 0.9393839836120605,
        "meteor": 0.6048387096774195,
        "sbert_similarity": 0.9304959774017334
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of painting did Caroline share with Melanie on October 13, 2023?",
      "prediction": "a painting of a sunset with a pink sky",
      "reference": "An abstract painting with blue streaks on a wall.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.375,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.2982797722714565,
        "bleu2": 0.057761629525894614,
        "bleu3": 0.03609050795723058,
        "bleu4": 0.028240990488565418,
        "bert_precision": 0.9109269976615906,
        "bert_recall": 0.9126874208450317,
        "bert_f1": 0.9118064045906067,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.5040774345397949
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Caroline attended about?",
      "prediction": "It was really powerful.",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.25,
        "rouge2_f": 0.14285714285714288,
        "rougeL_f": 0.25,
        "bleu1": 0.12113791079679323,
        "bleu2": 0.07819418518505664,
        "bleu3": 0.03513896216464457,
        "bleu4": 0.0253871431063062,
        "bert_precision": 0.9076462388038635,
        "bert_recall": 0.8546125292778015,
        "bert_f1": 0.8803313970565796,
        "meteor": 0.16741071428571427,
        "sbert_similarity": 0.1300111562013626
      }
    },
    {
      "sample_id": 0,
      "question": "What did the posters at the poetry reading say?",
      "prediction": "The context does not provide specific details about what the posters at the poetry reading said.",
      "reference": "\"Trans Lives Matter\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8081184029579163,
        "bert_recall": 0.8166956901550293,
        "bert_f1": 0.8123844265937805,
        "meteor": 0.0,
        "sbert_similarity": 0.14291472733020782
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's drawing symbolize for her?",
      "prediction": "freedom and being real",
      "reference": "Freedom and being true to herself.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6,
        "rouge1_f": 0.6,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6,
        "bleu1": 0.354274914555761,
        "bleu2": 0.3340135926488844,
        "bleu3": 0.2989505394514747,
        "bleu4": 0.1878296463217631,
        "bert_precision": 0.9402900338172913,
        "bert_recall": 0.9208530187606812,
        "bert_f1": 0.9304699897766113,
        "meteor": 0.5076628352490421,
        "sbert_similarity": 0.6762146949768066
      }
    },
    {
      "sample_id": 0,
      "question": "How do Melanie and Caroline describe their journey through life together?",
      "prediction": "We can make a real impact together!",
      "reference": "An ongoing adventure of learning and growing.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8695917725563049,
        "bert_recall": 0.869922935962677,
        "bert_f1": 0.869757354259491,
        "meteor": 0.0,
        "sbert_similarity": 0.15514971315860748
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Melanie's son on their road trip?",
      "prediction": "No relevant information found in the provided context.",
      "reference": "He got into an accident",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8139477968215942,
        "bert_recall": 0.8553661108016968,
        "bert_f1": 0.8341431617736816,
        "meteor": 0.0,
        "sbert_similarity": 0.11139344424009323
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's son handle the accident?",
      "prediction": "They were scared but reassured them and explained their brother would be OK. They're tough kids.",
      "reference": "He was scared but reassured by his family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.24,
        "rouge2_f": 0.17391304347826086,
        "rougeL_f": 0.24,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.140028008402801,
        "bleu3": 0.10943147363956583,
        "bleu4": 0.053463162573637084,
        "bert_precision": 0.8920114040374756,
        "bert_recall": 0.9008577466011047,
        "bert_f1": 0.8964127898216248,
        "meteor": 0.3345959595959596,
        "sbert_similarity": 0.7156779170036316
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family after the accident?",
      "prediction": "No relevant information was provided about an accident in the given context.",
      "reference": "They are important and mean the world to her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809525,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8362306356430054,
        "bert_recall": 0.8492085337638855,
        "bert_f1": 0.842669665813446,
        "meteor": 0.053763440860215055,
        "sbert_similarity": 0.04046978801488876
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's children handle the accident?",
      "prediction": "They were scared but were reassured and their brother was okay.",
      "reference": "They were scared but resilient",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5333333333333333,
        "rouge1_f": 0.5000000000000001,
        "rouge2_f": 0.4285714285714285,
        "rougeL_f": 0.5000000000000001,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.30151134457776363,
        "bleu3": 0.26648962915429364,
        "bleu4": 0.21200626759025185,
        "bert_precision": 0.8984586000442505,
        "bert_recall": 0.9249882698059082,
        "bert_f1": 0.9115304946899414,
        "meteor": 0.5636160714285715,
        "sbert_similarity": 0.649250864982605
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel after the accident?",
      "prediction": "Melanie felt scared during the accident and was grateful her son was okay.",
      "reference": "Grateful and thankful for her family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.0,
        "rougeL_f": 0.21052631578947367,
        "bleu1": 0.21428571428571425,
        "bleu2": 0.040599897147057515,
        "bleu3": 0.024859743585800658,
        "bleu4": 0.018798317647335087,
        "bert_precision": 0.8738040924072266,
        "bert_recall": 0.8708474636077881,
        "bert_f1": 0.8723232746124268,
        "meteor": 0.2238805970149254,
        "sbert_similarity": 0.4268902838230133
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's reaction to her children enjoying the Grand Canyon?",
      "prediction": "There is no mention of Melanie's reaction to her children enjoying the Grand Canyon in the provided context.",
      "reference": "She was happy and thankful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8435735106468201,
        "bert_recall": 0.8500877618789673,
        "bert_f1": 0.8468181490898132,
        "meteor": 0.0,
        "sbert_similarity": 0.2493814378976822
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's family give her?",
      "prediction": "A photograph of a plane flying in the sky with a star-filled sky",
      "reference": "Strength and motivation",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8141136169433594,
        "bert_recall": 0.8424974083900452,
        "bert_f1": 0.8280623555183411,
        "meteor": 0.0,
        "sbert_similarity": 0.02129034511744976
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family supporting her?",
      "prediction": "Thanks so much, Melanie! It's beautiful! It really brings home how much love's in families - both blood and the ones we choose.",
      "reference": "She appreciated them a lot",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8239860534667969,
        "bert_recall": 0.876893937587738,
        "bert_f1": 0.849617063999176,
        "meteor": 0.0,
        "sbert_similarity": 0.2245180755853653
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie do after the road trip to relax?",
      "prediction": "[Image: a photo of a car dashboard with a white cloth and a steering wheel] The roadtrip this past weekend was insane! We were all freaked when my son got into an accident. We were so lucky he was okay. It was a real scary experience. Thankfully it's over now.",
      "reference": "Went on a nature walk or hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.04081632653061224,
        "rouge1_f": 0.034482758620689655,
        "rouge2_f": 0.0,
        "rougeL_f": 0.034482758620689655,
        "bleu1": 0.016949152542372878,
        "bleu2": 0.005405800292019179,
        "bleu3": 0.003928700262818429,
        "bleu4": 0.0030932447039559207,
        "bert_precision": 0.8045884966850281,
        "bert_recall": 0.821814775466919,
        "bert_f1": 0.8131103515625,
        "meteor": 0.04424778761061947,
        "sbert_similarity": 0.3180882930755615
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline realize after her charity race?",
      "prediction": "Not mentioned in the conversation",
      "reference": "self-care is important",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.877884566783905,
        "bert_recall": 0.876802921295166,
        "bert_f1": 0.8773434162139893,
        "meteor": 0.0,
        "sbert_similarity": 0.10229768604040146
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's plans for the summer with respect to adoption?",
      "prediction": "Not mentioned in the conversation",
      "reference": "researching adoption agencies",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8703113198280334,
        "bert_recall": 0.8672592639923096,
        "bert_f1": 0.868782639503479,
        "meteor": 0.0,
        "sbert_similarity": 0.010256806388497353
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Melanie is considering support?",
      "prediction": "Not mentioned in the conversation",
      "reference": "LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.864730179309845,
        "bert_recall": 0.8227474689483643,
        "bert_f1": 0.843216598033905,
        "meteor": 0.0,
        "sbert_similarity": 0.12336073815822601
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose the adoption agency?",
      "prediction": "Not mentioned in the conversation",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.865925669670105,
        "bert_recall": 0.8060603141784668,
        "bert_f1": 0.8349213004112244,
        "meteor": 0.0,
        "sbert_similarity": 0.09916658699512482
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie excited about in her adoption process?",
      "prediction": "Not mentioned in the conversation",
      "reference": "creating a family for kids who need one",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8648307919502258,
        "bert_recall": 0.8169586658477783,
        "bert_f1": 0.840213418006897,
        "meteor": 0.0,
        "sbert_similarity": -0.060942042618989944
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie's necklace symbolize?",
      "prediction": "Not mentioned",
      "reference": "love, faith, and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8406462669372559,
        "bert_recall": 0.8283830881118774,
        "bert_f1": 0.8344696164131165,
        "meteor": 0.0,
        "sbert_similarity": 0.04912427440285683
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Melanie's grandma from?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Sweden",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8494402170181274,
        "bert_recall": 0.8910871744155884,
        "bert_f1": 0.8697654604911804,
        "meteor": 0.0,
        "sbert_similarity": 0.12741494178771973
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Melanie?",
      "prediction": "necklace",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandpa's gift to Caroline?",
      "prediction": "Not mentioned in the conversation",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8458927869796753,
        "bert_recall": 0.8909748792648315,
        "bert_f1": 0.8678487539291382,
        "meteor": 0.0,
        "sbert_similarity": 0.17644499242305756
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family do while camping?",
      "prediction": "explored nature, roasted marshmallows, and went on a hike",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Melanie interested in pursuing?",
      "prediction": "Not mentioned in the conversation",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8718169927597046,
        "bert_recall": 0.8251791596412659,
        "bert_f1": 0.8478571772575378,
        "meteor": 0.0,
        "sbert_similarity": 0.05818858742713928
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling workshop did Melanie attend recently?",
      "prediction": "LGBTQ+ counseling workshop",
      "reference": "LGBTQ+ counseling workshop",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Melanie to pursue counseling?",
      "prediction": "Not mentioned in the conversation",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.10526315789473682,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473682,
        "bleu1": 0.027067056647322542,
        "bleu2": 0.009569649651041096,
        "bleu3": 0.007666923387854731,
        "bleu4": 0.007271361304044331,
        "bert_precision": 0.8843756318092346,
        "bert_recall": 0.8464587330818176,
        "bert_f1": 0.8650018572807312,
        "meteor": 0.038167938931297704,
        "sbert_similarity": 0.004239174537360668
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Melanie want to create for people?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a safe and inviting place for people to grow",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8716058135032654,
        "bert_recall": 0.8357222080230713,
        "bert_f1": 0.853286862373352,
        "meteor": 0.0,
        "sbert_similarity": 0.10203901678323746
      }
    },
    {
      "sample_id": 0,
      "question": "Did Caroline make the black and white bowl in the photo?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8480530381202698,
        "bert_recall": 0.8904052376747131,
        "bert_f1": 0.8687132596969604,
        "meteor": 0.0,
        "sbert_similarity": 0.11724156141281128
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Caroline got used for?",
      "prediction": "Running",
      "reference": "Running",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's reason for getting into running?",
      "prediction": "Not mentioned in the conversation",
      "reference": "To de-stress and clear her mind",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8849430084228516,
        "bert_recall": 0.8652691841125488,
        "bert_f1": 0.8749955296516418,
        "meteor": 0.0,
        "sbert_similarity": 0.1525953859090805
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline say running has been great for?",
      "prediction": "Her mental health",
      "reference": "Her mental health",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie see at the council meeting for adoption?",
      "prediction": "Not mentioned in the conversation",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12500000000000003,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.8724703788757324,
        "bert_recall": 0.8342358469963074,
        "bert_f1": 0.8529248237609863,
        "meteor": 0.04807692307692308,
        "sbert_similarity": 0.013970711268484592
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Melanie's painting for the art show?",
      "prediction": "Not mentioned in the conversation",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8672130703926086,
        "bert_recall": 0.8251694440841675,
        "bert_f1": 0.8456690311431885,
        "meteor": 0.0,
        "sbert_similarity": -0.0016056214226409793
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's sculpture for the art show?",
      "prediction": "Not mentioned in the conversation",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8672130703926086,
        "bert_recall": 0.8251694440841675,
        "bert_f1": 0.8456690311431885,
        "meteor": 0.0,
        "sbert_similarity": -0.0016056214226409793
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Caroline go to the beach with her kids?",
      "prediction": "Not mentioned in the conversation",
      "reference": "once or twice a year",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8639419078826904,
        "bert_recall": 0.807560920715332,
        "bert_f1": 0.8348005414009094,
        "meteor": 0.0,
        "sbert_similarity": 0.053056638687849045
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family see during their camping trip last year?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Perseid meteor shower",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8497329354286194,
        "bert_recall": 0.7694872617721558,
        "bert_f1": 0.8076216578483582,
        "meteor": 0.0,
        "sbert_similarity": 0.033341649919748306
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel while watching the meteor shower?",
      "prediction": "in awe of the universe",
      "reference": "in awe of the universe",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose to use colors and patterns in her pottery project?",
      "prediction": "Not mentioned in the conversation",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.8825768828392029,
        "bert_recall": 0.8720123171806335,
        "bert_f1": 0.8772627711296082,
        "meteor": 0.052631578947368425,
        "sbert_similarity": 0.041106488555669785
      }
    },
    {
      "sample_id": 0,
      "question": "Is Oscar Melanie's pet?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8480530381202698,
        "bert_recall": 0.8904052376747131,
        "bert_f1": 0.8687132596969604,
        "meteor": 0.0,
        "sbert_similarity": 0.11724156141281128
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oscar hide his bone once?",
      "prediction": "Not mentioned in the conversation",
      "reference": "In Melanie's slipper",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8764923810958862,
        "bert_recall": 0.831631064414978,
        "bert_f1": 0.8534726500511169,
        "meteor": 0.15625,
        "sbert_similarity": 0.17263296246528625
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Melanie used to do with her dad?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Horseback riding",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8533901572227478,
        "bert_recall": 0.7734709978103638,
        "bert_f1": 0.8114676475524902,
        "meteor": 0.0,
        "sbert_similarity": 0.056279320269823074
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie make for a local church?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a stained glass window",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8690840601921082,
        "bert_recall": 0.8363252878189087,
        "bert_f1": 0.8523900508880615,
        "meteor": 0.0,
        "sbert_similarity": 0.03617002069950104
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie find in her neighborhood during her walk?",
      "prediction": "Not mentioned in the conversation or a rainbow sidewalk",
      "reference": "a rainbow sidewalk",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.28867513459481287,
        "bleu3": 0.23173297416566233,
        "bleu4": 0.11868405219520975,
        "bert_precision": 0.8754580020904541,
        "bert_recall": 0.9470217823982239,
        "bert_f1": 0.9098348617553711,
        "meteor": 0.8179012345679013,
        "sbert_similarity": 0.7155339121818542
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Melanie to be courageous?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Brave by Sara Bareilles",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8708803057670593,
        "bert_recall": 0.8365832567214966,
        "bert_f1": 0.8533873558044434,
        "meteor": 0.0,
        "sbert_similarity": -0.0043097082525491714
      }
    },
    {
      "sample_id": 0,
      "question": "What type of instrument does Caroline play?",
      "prediction": "Not mentioned in the conversation",
      "reference": "clarinet and violin",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8583569526672363,
        "bert_recall": 0.7918203473091125,
        "bert_f1": 0.8237472772598267,
        "meteor": 0.0,
        "sbert_similarity": 0.012888428755104542
      }
    },
    {
      "sample_id": 0,
      "question": "Which classical musicians does Caroline enjoy listening to?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Bach and Mozart",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8735336065292358,
        "bert_recall": 0.8279000520706177,
        "bert_f1": 0.8501048684120178,
        "meteor": 0.0,
        "sbert_similarity": 0.10292421281337738
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Caroline a fan of in terms of modern music?",
      "prediction": "Not mentioned in the conversation or Ed Sheeran",
      "reference": "Ed Sheeran",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.8623948693275452,
        "bert_recall": 0.9059621095657349,
        "bert_f1": 0.88364177942276,
        "meteor": 0.7211538461538463,
        "sbert_similarity": 0.7274355888366699
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Caroline see at the caf\u00e9?",
      "prediction": "Not mentioned in the conversation",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12500000000000003,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.8804165124893188,
        "bert_recall": 0.8307622671127319,
        "bert_f1": 0.8548689484596252,
        "meteor": 0.04807692307692308,
        "sbert_similarity": 0.09248805046081543
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Caroline face recently?",
      "prediction": "Not mentioned in the conversation",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.886946439743042,
        "bert_recall": 0.8621915578842163,
        "bert_f1": 0.874393880367279,
        "meteor": 0.0,
        "sbert_similarity": 0.037741709500551224
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline do to keep herself busy during her pottery break?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Read a book and paint.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8759547472000122,
        "bert_recall": 0.8547960519790649,
        "bert_f1": 0.865246057510376,
        "meteor": 0.0,
        "sbert_similarity": 0.08983369171619415
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Melanie attended about?",
      "prediction": "Not mentioned in the conversation",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8824484348297119,
        "bert_recall": 0.8453162908554077,
        "bert_f1": 0.8634833097457886,
        "meteor": 0.0,
        "sbert_similarity": 0.0863410234451294
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Caroline's son on their road trip?",
      "prediction": "He got into an accident",
      "reference": "He got into an accident",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.996,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's son handle the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "He was scared but reassured by his family",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.875566840171814,
        "bert_recall": 0.8454296588897705,
        "bert_f1": 0.8602343797683716,
        "meteor": 0.0,
        "sbert_similarity": 0.12110546976327896
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel about her family after the accident?",
      "prediction": "They are important and mean the world to her",
      "reference": "They are important and mean the world to her",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's children handle the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "They were scared but resilient",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8781699538230896,
        "bert_recall": 0.8506824970245361,
        "bert_f1": 0.8642076849937439,
        "meteor": 0.0,
        "sbert_similarity": 0.07462236285209656
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel after the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Grateful and thankful for her family",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705779314041138,
        "bert_recall": 0.819611668586731,
        "bert_f1": 0.844326376914978,
        "meteor": 0.0,
        "sbert_similarity": 0.012023890390992165
      }
    },
    {
      "sample_id": 0,
      "question": "What was Caroline's reaction to her children enjoying the Grand Canyon?",
      "prediction": "She was happy and thankful",
      "reference": "She was happy and thankful",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline do after the road trip to relax?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Went on a nature walk or hike",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8826772570610046,
        "bert_recall": 0.8326032161712646,
        "bert_f1": 0.8569092750549316,
        "meteor": 0.0,
        "sbert_similarity": 0.0687059536576271
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline love most about camping with her family?",
      "prediction": "Being present and bonding with her family",
      "reference": "Being present and bonding with her family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0000001192092896
      }
    }
  ]
}